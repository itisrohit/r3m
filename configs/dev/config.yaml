# R3M Development Configuration - Core Document Processing Pipeline
server:
  port: 8080
  host: "0.0.0.0"
  threads: 4

logging:
  level: "debug"
  format: "text"
  output: "/app/data/logs/r3m.log"

# Document Processing Pipeline Configuration (Core Formats Only)
document_processing:
  # Supported file extensions (core formats only)
  supported_extensions:
    plain_text: [".txt", ".md", ".mdx", ".conf", ".log", ".json", ".csv", ".tsv", ".xml", ".yml", ".yaml"]
    pdf: [".pdf"]
    html: [".html", ".htm"]
  
  # Processing settings
  max_file_size: "100MB"
  max_text_length: 1000000  # 1M characters
  enable_parallel_processing: true
  worker_threads: 4
  
  # OPTIMIZED BATCH PROCESSING SETTINGS
  batch_size: 16  # Optimal batch size for efficient processing
  max_workers: 4  # Optimal worker count for most systems
  
  # OPTIMIZED PARALLEL PROCESSING CONFIGURATION
  enable_optimized_thread_pool: true
  enable_thread_affinity: true
  enable_work_stealing: true
  enable_memory_pooling: true
  
  # SIMD OPTIMIZATION CONFIGURATION
  enable_simd_optimizations: true
  enable_avx2: true
  enable_avx512: true
  enable_neon: true
  
  # CHUNKING CONFIGURATION - OPTIMIZED!
  enable_chunking: true
  
  # Text processing
  text_processing:
    encoding_detection: true
    default_encoding: "utf-8"
    remove_html_tags: true
    normalize_whitespace: true
    extract_metadata: true
  
  # Quality filtering (advanced document processing feature)
  quality_filtering:
    enabled: true
    min_content_quality_score: 0.3      # Minimum quality score (0.0-1.0)
    min_information_density: 0.1        # Minimum information density
    min_content_length: 50              # Minimum content length
    max_content_length: 1000000         # Maximum content length
    filter_empty_documents: true        # Filter out empty documents
    filter_low_quality_documents: true  # Filter out low-quality documents
    
    # Quality assessment algorithm weights
    quality_weights:
      length_factor: 0.3                # Length factor weight (0-1.0)
      word_diversity_factor: 0.3        # Word diversity factor weight (0-1.0)
      sentence_structure_factor: 0.2    # Sentence structure factor weight (0-1.0)
      information_density_factor: 0.2   # Information density factor weight (0-1.0)
    
    # Information density calculation weights
    density_weights:
      unique_word_ratio: 0.4            # Unique word ratio weight (0-1.0)
      technical_term_density: 0.3       # Technical term density weight (0-1.0)
      sentence_complexity: 0.3          # Sentence complexity weight (0-1.0)
    
    # Quality calculation thresholds
    quality_thresholds:
      length_normalization: 1000        # Text length for normalization
      word_diversity_normalization: 5   # Characters per word for diversity calculation
      sentence_normalization: 10        # Number of sentences for normalization
      technical_term_normalization: 10  # Characters per technical term
      sentence_complexity_normalization: 100  # Average sentence length for complexity
      whitespace_threshold: 0.1         # Minimum non-whitespace ratio
  
  # Pipeline stages (core processing only)
  pipeline_stages:
    - "file_validation"
    - "text_extraction"
    - "text_cleaning"
    - "metadata_extraction"
    - "quality_assessment"
    - "filtering"
    - "chunking"

# Chunking Configuration (Advanced Document Segmentation) - OPTIMIZED!
chunking:
  # Tokenizer settings
  tokenizer:
    type: "basic"                    # Tokenizer type: basic, bpe, sentencepiece
    max_tokens: 8192                 # Maximum tokens per chunk
    token_limit: 2048                # Target token limit for chunks
    chunk_overlap: 0                 # Token overlap between chunks (0 for clean combinations)
  
  # OPTIMIZED CHUNK SIZE SETTINGS
  chunk_sizes:
    blurb_size: 100                  # Size for extracting blurbs (first sentence)
    mini_chunk_size: 150             # Mini-chunk size for multipass mode
    large_chunk_ratio: 4             # Number of regular chunks per large chunk
    chunk_min_content: 256           # Minimum content tokens per chunk
  
  # OPTIMIZED METADATA SETTINGS
  metadata:
    include_metadata: true            # Include document metadata in chunks
    max_metadata_percentage: 0.25    # Maximum metadata percentage (25%)
    skip_metadata_in_chunk: false    # Skip metadata in chunk content
  
  # OPTIMIZED CHUNKING FEATURES
  enable_multipass: true             # Enable multi-pass indexing
  enable_large_chunks: true          # Enable large chunk generation
  enable_contextual_rag: true        # Enable contextual RAG features
  contextual_rag_reserved_tokens: 512 # Reserved tokens for RAG context
  
  # OPTIMIZED TOKEN PROCESSING
  enable_token_caching: true         # Enable token caching for performance
  enable_string_view_optimization: true # Enable string_view optimizations
  enable_preallocation: true         # Enable vector preallocation
  enable_move_semantics: true        # Enable move semantics for efficiency

# Engine configuration
engine:
  # Performance settings
  max_memory_mb: 2048
  cache_memory_mb: 512
  batch_timeout_seconds: 30
  
  # Thread pool settings
  thread_pool:
    fallback_threads: 4                 # Fallback thread count if auto-detection fails
    hardware_concurrency_fallback: 4    # Fallback for hardware_concurrency()
  
  # Monitoring settings
  enable_metrics: true
  metrics_interval_seconds: 5

# Storage paths
storage:
  data_path: "/app/data"
  temp_path: "/tmp/r3m"
  cache_path: "/app/data/cache"

# Test configuration
testing:
  # Test file creation parameters
  test_files:
    large_file_count: 8                 # Number of large files for parallel testing
    large_file_lines: 500               # Lines per large file
    enhancement_file_count: 12          # Number of files for enhancement testing
    enhancement_file_lines: 300         # Lines per enhancement file
  
  # Test performance parameters
  performance:
    efficiency_thread_count: 4          # Thread count for efficiency calculation
    test_batch_size: 4                  # Batch size for testing
    test_max_workers: 4                 # Max workers for testing 