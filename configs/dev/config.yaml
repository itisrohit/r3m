# R3M Development Configuration - Core Document Processing Pipeline
server:
  port: 8080
  host: "0.0.0.0"
  threads: 4

logging:
  level: "debug"
  format: "text"
  output: "/app/data/logs/r3m.log"

# Document Processing Pipeline Configuration (Core Formats Only)
document_processing:
  # Supported file extensions (core formats only)
  supported_extensions:
    plain_text: [".txt", ".md", ".mdx", ".conf", ".log", ".json", ".csv", ".tsv", ".xml", ".yml", ".yaml"]
    pdf: [".pdf"]
    html: [".html", ".htm"]
  
  # Processing settings
  max_file_size: "100MB"
  max_text_length: 1000000  # 1M characters
  enable_parallel_processing: true
  worker_threads: 4
  
  # Batch processing settings (following modern document processing patterns)
  batch_size: 16  # Default batch size for efficient processing
  max_workers: 8  # Maximum worker threads (auto-detected if not set)
  
  # Text processing
  text_processing:
    encoding_detection: true
    default_encoding: "utf-8"
    remove_html_tags: true
    normalize_whitespace: true
    extract_metadata: true
  
  # Quality filtering (advanced document processing feature)
  quality_filtering:
    enabled: true
    min_content_quality_score: 0.3      # Minimum quality score (0.0-1.0)
    min_information_density: 0.1        # Minimum information density
    min_content_length: 50              # Minimum content length
    max_content_length: 1000000         # Maximum content length
    filter_empty_documents: true        # Filter out empty documents
    filter_low_quality_documents: true  # Filter out low-quality documents
    
    # Quality assessment algorithm weights
    quality_weights:
      length_factor: 0.3                # Length factor weight (0-1.0)
      word_diversity_factor: 0.3        # Word diversity factor weight (0-1.0)
      sentence_structure_factor: 0.2    # Sentence structure factor weight (0-1.0)
      information_density_factor: 0.2   # Information density factor weight (0-1.0)
    
    # Information density calculation weights
    density_weights:
      unique_word_ratio: 0.4            # Unique word ratio weight (0-1.0)
      technical_term_density: 0.3       # Technical term density weight (0-1.0)
      sentence_complexity: 0.3          # Sentence complexity weight (0-1.0)
    
    # Quality calculation thresholds
    quality_thresholds:
      length_normalization: 1000        # Text length for normalization
      word_diversity_normalization: 5   # Characters per word for diversity calculation
      sentence_normalization: 10        # Number of sentences for normalization
      technical_term_normalization: 10  # Characters per technical term
      sentence_complexity_normalization: 100  # Average sentence length for complexity
      whitespace_threshold: 0.1         # Minimum non-whitespace ratio
  
  # Pipeline stages (core processing only)
  pipeline_stages:
    - "file_validation"
    - "text_extraction"
    - "text_cleaning"
    - "metadata_extraction"
    - "quality_assessment"
    - "filtering"
    - "chunking"

# Chunking Configuration (Advanced Document Segmentation)
chunking:
  # Tokenizer settings
  tokenizer:
    type: "basic"                    # Tokenizer type: basic, bpe, sentencepiece
    max_tokens: 8192                 # Maximum tokens per chunk
    token_limit: 2048                # Target token limit for chunks
    chunk_overlap: 0                 # Token overlap between chunks (0 for clean combinations)
  
  # Chunk size settings
  chunk_sizes:
    blurb_size: 100                  # Size for extracting blurbs (first sentence)
    mini_chunk_size: 150             # Mini-chunk size for multipass mode
    large_chunk_ratio: 4             # Number of regular chunks per large chunk
    chunk_min_content: 256           # Minimum content tokens per chunk
  
  # Metadata settings
  metadata:
    include_metadata: true            # Include document metadata in chunks
    max_metadata_percentage: 0.25    # Maximum metadata percentage (25%)
    skip_metadata_in_chunk: false    # Skip metadata in chunk content
  
  # Advanced features
  advanced_features:
    enable_multipass: false          # Enable multipass indexing
    enable_large_chunks: false       # Enable large chunk generation
    enable_contextual_rag: false     # Enable contextual RAG (requires LLM)
    
    # Contextual RAG settings
    contextual_rag:
      reserved_tokens: 512           # Tokens reserved for contextual information
      use_document_summary: true     # Use document-level summaries
      use_chunk_summary: true        # Use chunk-level summaries
      llm_model: "gpt-4o-mini"      # LLM model for summaries
      llm_provider: "openai"         # LLM provider
  
  # Quality filtering for chunks
  quality_filtering:
    enabled: true
    min_chunk_quality: 0.3           # Minimum chunk quality score
    min_chunk_density: 0.1           # Minimum information density
    min_chunk_length: 50             # Minimum chunk length (characters)
    max_chunk_length: 10000          # Maximum chunk length (characters)
  
  # Processing settings
  processing:
    enable_parallel_chunking: true   # Enable parallel chunk processing
    chunk_worker_threads: 4          # Number of worker threads for chunking
    batch_chunk_size: 16             # Batch size for chunk processing

# Engine configuration
engine:
  # Performance settings
  max_memory_mb: 2048
  cache_memory_mb: 512
  batch_timeout_seconds: 30
  
  # Thread pool settings
  thread_pool:
    fallback_threads: 4                 # Fallback thread count if auto-detection fails
    hardware_concurrency_fallback: 4    # Fallback for hardware_concurrency()
  
  # Monitoring settings
  enable_metrics: true
  metrics_interval_seconds: 5

# Storage paths
storage:
  data_path: "/app/data"
  temp_path: "/tmp/r3m"
  cache_path: "/app/data/cache"

# Test configuration
testing:
  # Test file creation parameters
  test_files:
    large_file_count: 8                 # Number of large files for parallel testing
    large_file_lines: 500               # Lines per large file
    enhancement_file_count: 12          # Number of files for enhancement testing
    enhancement_file_lines: 300         # Lines per enhancement file
  
  # Test performance parameters
  performance:
    efficiency_thread_count: 4          # Thread count for efficiency calculation
    test_batch_size: 4                  # Batch size for testing
    test_max_workers: 4                 # Max workers for testing 